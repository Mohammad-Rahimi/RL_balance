{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37ceffd-07b4-465d-a0a4-6ea62354b1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyoSuite:> Registering Myo Envs\n",
      "\u001b[36m    MyoSuite: A contact-rich simulation suite for musculoskeletal motor control\n",
      "        Vittorio Caggiano, Huawei Wang, Guillaume Durandau, Massimo Sartori, Vikash Kumar\n",
      "        L4DC-2019 | https://sites.google.com/view/myosuite\n",
      "    \u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rahimig.mohammad\\.conda\\envs\\MyoSuite2\\lib\\site-packages\\gymnasium\\spaces\\box.py:235: UserWarning: \u001b[33mWARN: Box low's precision lowered by casting to float32, current low.dtype=float64\u001b[0m\n",
      "  gym.logger.warn(\n",
      "C:\\Users\\rahimig.mohammad\\.conda\\envs\\MyoSuite2\\lib\\site-packages\\gymnasium\\spaces\\box.py:305: UserWarning: \u001b[33mWARN: Box high's precision lowered by casting to float32, current high.dtype=float64\u001b[0m\n",
      "  gym.logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to ./ppo_myo_tensorboard/PPO_5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rahimig.mohammad\\.conda\\envs\\MyoSuite2\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 36.9     |\n",
      "|    ep_rew_mean     | 39.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 392      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 36.7        |\n",
      "|    ep_rew_mean          | 40.1        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 324         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019293195 |\n",
      "|    clip_fraction        | 0.259       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -114        |\n",
      "|    explained_variance   | -0.000513   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.1e+03     |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0792     |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 1.98e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 36.4        |\n",
      "|    ep_rew_mean          | 39.1        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 307         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 20          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022219455 |\n",
      "|    clip_fraction        | 0.263       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -114        |\n",
      "|    explained_variance   | 0.317       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 674         |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.084      |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 1.42e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 36.3        |\n",
      "|    ep_rew_mean          | 36.4        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 300         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 27          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029301962 |\n",
      "|    clip_fraction        | 0.35        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -114        |\n",
      "|    explained_variance   | 0.482       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 549         |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0941     |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 1.14e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 36.2        |\n",
      "|    ep_rew_mean          | 36.8        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 296         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 34          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035196908 |\n",
      "|    clip_fraction        | 0.406       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -114        |\n",
      "|    explained_variance   | 0.593       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 380         |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0976     |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 966         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 36.4        |\n",
      "|    ep_rew_mean          | 35.7        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 293         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 41          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035635825 |\n",
      "|    clip_fraction        | 0.41        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -114        |\n",
      "|    explained_variance   | 0.667       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 382         |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0981     |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 815         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 36.4       |\n",
      "|    ep_rew_mean          | 37.9       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 290        |\n",
      "|    iterations           | 7          |\n",
      "|    time_elapsed         | 49         |\n",
      "|    total_timesteps      | 14336      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04031598 |\n",
      "|    clip_fraction        | 0.434      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -114       |\n",
      "|    explained_variance   | 0.723      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 316        |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | -0.0985    |\n",
      "|    std                  | 1          |\n",
      "|    value_loss           | 673        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 36.5        |\n",
      "|    ep_rew_mean          | 38.3        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 289         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 56          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039144456 |\n",
      "|    clip_fraction        | 0.446       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -114        |\n",
      "|    explained_variance   | 0.766       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 334         |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.093      |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 585         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 36.8       |\n",
      "|    ep_rew_mean          | 37         |\n",
      "| time/                   |            |\n",
      "|    fps                  | 288        |\n",
      "|    iterations           | 9          |\n",
      "|    time_elapsed         | 63         |\n",
      "|    total_timesteps      | 18432      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04318443 |\n",
      "|    clip_fraction        | 0.439      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -113       |\n",
      "|    explained_variance   | 0.814      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 250        |\n",
      "|    n_updates            | 80         |\n",
      "|    policy_gradient_loss | -0.0935    |\n",
      "|    std                  | 0.999      |\n",
      "|    value_loss           | 472        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 36.6        |\n",
      "|    ep_rew_mean          | 40          |\n",
      "| time/                   |             |\n",
      "|    fps                  | 288         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 71          |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.041248746 |\n",
      "|    clip_fraction        | 0.464       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -113        |\n",
      "|    explained_variance   | 0.844       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 197         |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0944     |\n",
      "|    std                  | 0.998       |\n",
      "|    value_loss           | 424         |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 36.7      |\n",
      "|    ep_rew_mean          | 42.7      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 287       |\n",
      "|    iterations           | 11        |\n",
      "|    time_elapsed         | 78        |\n",
      "|    total_timesteps      | 22528     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0487982 |\n",
      "|    clip_fraction        | 0.47      |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -113      |\n",
      "|    explained_variance   | 0.865     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 110       |\n",
      "|    n_updates            | 100       |\n",
      "|    policy_gradient_loss | -0.0946   |\n",
      "|    std                  | 0.998     |\n",
      "|    value_loss           | 333       |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 36.4        |\n",
      "|    ep_rew_mean          | 42.3        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 286         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 85          |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.050840158 |\n",
      "|    clip_fraction        | 0.486       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -113        |\n",
      "|    explained_variance   | 0.871       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 175         |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0932     |\n",
      "|    std                  | 0.998       |\n",
      "|    value_loss           | 287         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 36.1        |\n",
      "|    ep_rew_mean          | 44.2        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 286         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 92          |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.057455283 |\n",
      "|    clip_fraction        | 0.506       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -113        |\n",
      "|    explained_variance   | 0.906       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 87.9        |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0913     |\n",
      "|    std                  | 0.998       |\n",
      "|    value_loss           | 236         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 35.9        |\n",
      "|    ep_rew_mean          | 43.2        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 286         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 100         |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.055156182 |\n",
      "|    clip_fraction        | 0.479       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -113        |\n",
      "|    explained_variance   | 0.915       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 69.6        |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0888     |\n",
      "|    std                  | 0.998       |\n",
      "|    value_loss           | 192         |\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from myosuite.utils import gym\n",
    "import myosuite\n",
    "#import deprl\n",
    "#from deprl import env_wrappers\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "import torch\n",
    "\n",
    "# Create and wrap the environment\n",
    "\n",
    "#num_envs = 8\n",
    "#env = make_vec_env(env_id, n_envs=num_envs)\n",
    "env = gym.make('myoLegWalk-v0', reset_type='random')\n",
    "\n",
    "# Define the PPO algorithm\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = PPO(\n",
    "    'MlpPolicy',\n",
    "    env,\n",
    "    verbose=1,\n",
    "    tensorboard_log=\"./ppo_myo_tensorboard/\",\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "timesteps = 1_000_000\n",
    "model.learn(total_timesteps=timesteps)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"ppo_myo_model2\")\n",
    "\n",
    "# Evaluate the trained model\n",
    "env = gym.make('myoLegWalk-v0')\n",
    "obs = env.reset()\n",
    "for _ in range(1000):\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7101c4f1-37eb-4aee-8ff0-fe86e35982a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You have passed a tuple to the predict() function instead of a Numpy array or a Dict. You are probably mixing Gym API with SB3 VecEnv API: `obs, info = env.reset()` (Gym) vs `obs = vec_env.reset()` (SB3 VecEnv). See related issue https://github.com/DLR-RM/stable-baselines3/issues/1694 and documentation for more information: https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecenv-api-vs-gym-api",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m obs \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1000\u001b[39m):\n\u001b[1;32m----> 5\u001b[0m     action, _states \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     obs, rewards, dones, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m      7\u001b[0m     env\u001b[38;5;241m.\u001b[39mrender()\n",
      "File \u001b[1;32m~\\.conda\\envs\\MyoSuite2\\lib\\site-packages\\stable_baselines3\\common\\base_class.py:557\u001b[0m, in \u001b[0;36mBaseAlgorithm.predict\u001b[1;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    539\u001b[0m     observation: Union[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray]],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    542\u001b[0m     deterministic: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    543\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[np\u001b[38;5;241m.\u001b[39mndarray, Optional[\u001b[38;5;28mtuple\u001b[39m[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]]]:\n\u001b[0;32m    544\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    545\u001b[0m \u001b[38;5;124;03m    Get the policy action from an observation (and optional hidden state).\u001b[39;00m\n\u001b[0;32m    546\u001b[0m \u001b[38;5;124;03m    Includes sugar-coating to handle different observations (e.g. normalizing images).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    555\u001b[0m \u001b[38;5;124;03m        (used in recurrent policies)\u001b[39;00m\n\u001b[0;32m    556\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 557\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisode_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\MyoSuite2\\lib\\site-packages\\stable_baselines3\\common\\policies.py:357\u001b[0m, in \u001b[0;36mBasePolicy.predict\u001b[1;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;66;03m# Check for common mistake that the user does not mix Gym/VecEnv API\u001b[39;00m\n\u001b[0;32m    355\u001b[0m \u001b[38;5;66;03m# Tuple obs are not supported by SB3, so we can safely do that check\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(observation, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(observation) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(observation[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m--> 357\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    358\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have passed a tuple to the predict() function instead of a Numpy array or a Dict. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    359\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are probably mixing Gym API with SB3 VecEnv API: `obs, info = env.reset()` (Gym) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    360\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvs `obs = vec_env.reset()` (SB3 VecEnv). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    361\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSee related issue https://github.com/DLR-RM/stable-baselines3/issues/1694 \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    362\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand documentation for more information: https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecenv-api-vs-gym-api\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    363\u001b[0m     )\n\u001b[0;32m    365\u001b[0m obs_tensor, vectorized_env \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs_to_tensor(observation)\n\u001b[0;32m    367\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m th\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "\u001b[1;31mValueError\u001b[0m: You have passed a tuple to the predict() function instead of a Numpy array or a Dict. You are probably mixing Gym API with SB3 VecEnv API: `obs, info = env.reset()` (Gym) vs `obs = vec_env.reset()` (SB3 VecEnv). See related issue https://github.com/DLR-RM/stable-baselines3/issues/1694 and documentation for more information: https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecenv-api-vs-gym-api"
     ]
    }
   ],
   "source": [
    "# Evaluate the trained model\n",
    "env = gym.make('myoLegWalk-v0')\n",
    "obs = env.reset()\n",
    "for _ in range(1000):\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd06cd4-8d20-4271-b7ec-710f86f389bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
